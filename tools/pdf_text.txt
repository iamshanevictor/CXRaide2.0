Automatic Chest X-Ray Pattern Annotation and Classification ABSTRACT Errors in chest X-ray (CXR) interpretation can delay treatment and negatively impact patient outcomes. Previous studies lacked a systematic approach to storing radiologist annotations with precise coordinates, limiting AI validation and accurate abnormality local- ization. To address these challenges, CXRaide 2.0 was developed with three key objectives: enhancing the annotation tool to save radiologist annotations and bounding box data, improving abnor- mality classification to reduce misclassification and background detection, and generating structured radiographic reports with clear descriptions and abnormality locations. The SSD300_VGG16 model was trained on NIH and VinBig datasets using iterative develop- ment, data preprocessing, and optimized sampling. The annotation tool was enhanced to capture bounding box coordinates, while a template-based approach was implemented for automated re- port generation. The model achieved a moderate mAP (31.02%) with high recall (83.6–91.4%), effectively detecting abnormalities. However, precision declined at stricter localization thresholds, high- lighting the need for improved bounding box regression. CXRaide 2.0 demonstrated improvements in recall and classification accu- racy over its predecessor but requires further refinement in preci- sion and loss reduction. Future enhancements include improving classification accuracy, addressing image size inconsistencies, and integrating NLP for automated report generation, ensuring better clinical reliability and effectiveness. CCS CONCEPTS •Computing methodologies → Machine learning approaches; Neural networks; Computer vision; •Applied computing → Health informatics; Imaging; KEYWORDS SSD300_VGG16, Chest X-ray, AI, Machine Learning, Image Anno- tation, Abnormality Detection, Radiographic Report 1 INTRODUCTION Errors in diagnostic radiology can lead to treatment delays, poor patient outcomes, increased healthcare costs, lost revenue, and op- erational inefficiencies. Despite this, over 20% of radiology reports contain inaccuracies. On average, the diagnostic error rate in these reports is estimated to be between 3% and 5%, resulting in roughly 40 million diagnostic imaging errors annually. With millions of images generated annually, chest radiographs (CXR) are crucial for diagnosing conditions such as rib fractures, pneumothoraces, and pulmonary infections [6]. Recent technological advancements have enabled the development of systems that enhance radiologists’ efficiency and accuracy in an- alyzing CXR images. Consequently, there has been growing interest in creating AI-based systems for detecting and prioritizing CXR findings, as well as efficiently collecting training data. According to [15], labeled data are essential for training, validating, and testing a deep learning algorithm. Consistent labeling has been achieved through previously developed ontological schemes, ranging from hierarchical systems with over 180 unique labels to simpler ones with only a few selected labels. CXR training data should include manually labeled findings on radiographic images, often marked with bounding boxes to indicate locations. Radiologists are typi- cally required for this task to ensure the highest accuracy. As a result, collecting data for training an algorithm can be both time- consuming and expensive. To address this, several systems have been developed for automatically extracting labels from CXR text reports, using natural language processing models based on either feature engineering or deep learning technology [9]. Recently, contrastive learning has shown promise in leverag- ing unlabeled images for generalizable features, but its applica- tion to medical imaging remains challenging. [5] proposed a semi- supervised knowledge-augmented contrastive learning framework for disease classification and localization. By integrating radiomic features for positive sampling, it enhances robustness and inter- pretability. Experiments on the NIH Chest X-ray dataset show su- perior performance over existing baselines. Automatic detection of findings and their locations in chest x-ray studies is crucial for AI ap- plication in healthcare. For finding classification tasks, image-level labeling suffices, but additional annotation with bounding boxes is required for the detection of the findings’ locations. This process is time-consuming and costly, as it needs to be performed by radiolo- gists. To address this problem, weakly supervised approaches have been used to depict finding locations by looking at attention maps produced by convolution networks trained for finding classification. Automatic detection of findings and their locations in chest x- ray studies is crucial for AI application in healthcare. For finding classification tasks, image-level labeling suffices, but additional an- notation with bounding boxes is required for detection of findings’ locations. This process is time-consuming and costly, as it needs to be performed by radiologists. To address this problem, weakly supervised approaches have been used to depict finding locations by looking at attention maps produced by convolution networks trained for finding classification. [16] proposed an automatic ap- proach for labeling chest x-ray images for findings and locations by leveraging radiology reports, using a lung segmentation UNet model and an atlas of normal patients to mark six lung zones with standardized bounding boxes and label each lung zone as positive or negative for findings. Reading and interpreting chest X-ray images can be challenging even for experienced radiologists. [3] proposed a multi-model deep learning-based automated chest X-ray report generator system to assist radiologists. This system employs a two-step approach: first, it detects abnormalities in CXR images, localizing each one with a bounding box and a probability score; second, a fine-tuned large language model (LLM) converts these detected abnormalities and

PCSC2025, May 2025, Baguio, Philippines their probabilities into textual descriptions for the findings section of the report. They utilized the RadBloomz model, a variant of the GPT-powered Bloomz-7b1 model, adapted to the radiology domain through domain-adapted pre-training on the MIMIC-IV dataset. The experimental results showed a better ROUGE-L score than the competing models, with an absolute improvement of 8.4 percentage points. An annotation model for abnormality detection and bounding box placement in chest X-ray images was developed using Efficient- Det, as explored in [2]. Performance evaluation showed improved detection accuracy over multiple iterations, but precise localization remained challenging, especially at higher IoU thresholds. Despite enhancements, the model struggled with accurate bounding box placement, limiting its clinical applicability. This research high- lights EfficientDet’s potential in medical image annotation while emphasizing the need for further optimization to improve localiza- tion accuracy. 1.0.1 Objectives. The key objectives of this study are to enhance the annotation tool by ensuring that radiologist-made annotations are saved for record-keeping purposes, with expert-annotated images and their corresponding bounding box placements and coordinates stored in a CSV file. Additionally, the study aims to improve the abnor- mality classification system to reduce misclassifications and avoid detecting irrelevant background areas, thereby enhancing the sys- tem’s accuracy in identifying and classifying abnormalities while minimizing errors and improving diagnostic precision. Lastly, the study seeks to refine the automatic interpretation of findings to generate radiographic reports that closely follow traditional for- mats, incorporating clear, sentence-form descriptions with a focus on the precise locations of abnormalities. 2 RELATED WORKS A system was developed to address the challenge of radiologists being overloaded with chest X-ray (CXR) exams, which can delay accurate report generation. The study aimed to automate radiology report writing using RadioBERT, a model that integrates Distil- BERT for natural language processing and a CNN for extracting key image details. These details are processed through a hierarchi- cal LSTM to generate structured reports, prioritizing abnormalities. The findings indicate that leveraging DistilBERT improves clinical report accuracy and efficiency [8]. Deep learning automates medical imaging report generation, as explored in [10], using an encoder-decoder framework where a CNN extracts visual features and an RNN (LSTM/GRU) generates text. Techniques like attention mechanisms, reinforcement learn- ing, and GANs enhance quality, while hierarchical RNNs structure reports from summaries to detailed descriptions. Despite improve- ments in efficiency and accuracy, challenges in interpretability and clinical applicability persist. AI-driven chest X-ray (CXR) analysis has been extensively re- viewed in [1], covering key tasks, lung diseases, and challenges in medical diagnosis. Researchers have explored datasets, evalua- tion metrics, and patents while highlighting the role of computer vision and machine learning in radiology. Topics include image pre-processing, classification, disease localization, report genera- tion, and model explainability. AI applications in tuberculosis (TB), pneumoconiosis, pneumonia, and COVID-19 detection are exam- ined, along with issues related to data availability, privacy, and interpretability. Traditional and deep learning-based approaches are compared, offering insights into advancements and remaining challenges. A two-stage multicenter study in [13] evaluates the real-world clinical performance of Lunit INSIGHT CXR, a commercial AI al- gorithm for chest X-ray (CXR) analysis. The study includes a ret- rospective case-control study and a prospective validation study, assessing diagnostic accuracy while ensuring external validation and real-world applicability. AI performance is compared with hu- man radiologists using key metrics such as AUC, sensitivity, and specificity. An error analysis examines issues related to poor input data and human-AI interaction. Despite advancements, challenges remain, including the impact of clinically insignificant findings and the need for better quality control in CXR imaging. IVGG13, an improved VGG16 model for pneumonia X-ray clas- sification, enhances Convolutional Neural Networks (CNNs) by reducing network depth to prevent overfitting while preserving strong feature extraction. The model employs smaller convolu- tional kernels and an input size of 128 ×128, structured into five blocks, each containing two convolutional layers and a pooling layer. Data augmentation mitigates dataset imbalance, improving recognition accuracy. Evaluated on Kaggle’s pediatric thoracic X- ray dataset, IVGG13 outperformed CNN models like LeNet, AlexNet, and GoogLeNet, demonstrating superior precision, recall, and F1- measure. Additionally, the model improves hardware efficiency, reducing computational load without compromising accuracy [7]. An AI-assisted system for chest X-ray (CXR) annotation was developed to aid radiologists in abnormality classification and lo- calization. Using EfficientDet and Django frameworks, the system enhances CXR interpretation by detecting abnormalities and refin- ing annotations through a customizable radiography tool. The study utilized the NIH CheXpert and Vin-DR CXR datasets, focusing on 9 common abnormalities. Images were preprocessed, resized to 512x512 pixels, and annotations were merged using Weighted Box Fusion (WBF). The EfficientDet D0 model was trained for 1300 steps, with the lowest training loss observed at 1200 steps, suggesting an optimal stopping point. Preliminary results indicate moderate accuracy. The overall Average Precision (AP) was 0.104, with an AP of 0.275 at IoU 0.50 and 0.050 at IoU 0.75, highlighting challenges in precise localization. The model performed best for large objects (AP = 0.146) but struggled with small objects (AP = 0.008). Recall (AR) ranged from 0.174 to 0.349, favoring larger abnormalities. AUC scores varied, with Pleural Effusion (0.63) and Infiltration (0.58) performing best, while Pneumothorax (0.50), Consolidation (0.47), and Atelectasis (0.46) had the lowest scores. These results suggest room for improvement, particularly in precision, recall, and small abnormality detection. The moderate performance may stem from

Automatic Chest X-Ray Pattern Annotation and Classification PCSC2025, May 2025, Baguio, Philippines dataset complexity, model limitations, or insufficient training. Fu- ture work will focus on data quality, radiologist feedback, additional training, and hyperparameter tuning to enhance clinical viability. This AI-assisted system represents a step forward in radiology, with the potential to improve diagnostic accuracy and efficiency [11]. Table 1: Comparative Study of Related Works in Chest X-ray Annotation and Classification Study Tool Type Annotation Type Model Extraction Feature Evaluation Metrics Reported Results [4] (2014) Annotation Tool Fully automatic (visual features + SVM) SVM (binary) ZM, GLCM, DCT + PCA Accuracy, Sensitiv- ity, Speci- ficity Acc: 96.56%, Sens: 96.30%, Spec: 96.60% [17] (2022) Annotation and Clas- sification Semi- auto (LSTM + DenseNet) DenseNet (7-class) Visual (DenseNet), Textual (LSTM) F1-score, AUC F1: 0.86, AUC: 0.835, Recall: 0.85, Precision: 0.88 [18] (2014) Annotation and Clas- sification Fully automatic (PLSA + SVM) Weighted keyword matching Visual BoW, Text Topics (PLSA) Accuracy, Precision, Recall, F1 Acc: 87.5%, F1: 0.93, Precision: 0.94 [12] (2021) Radiographic Report Fully automatic BERT (fine- tuned) Textual (radiol- ogy reports) F1-score (13 diseases) F1: 0.798 [14] (2018) Annotation Tool + Radio- graphic Report NLP- mined and hand- labeled annota- tions (ChestX- ray14, OpenI) CNN- RNN (ResNet- 50 + LSTM with attention) Visual (SW- GAP), Textual (AETE) Accuracy, Sensitiv- ity, Speci- ficity, AUC, BLEU scores, METEOR, and ROUGE-L Acc: 96.56%, Sens: 96.30%, Spec: 96.60%, BLEU↑, ME- TEOR↑, ROUGE- L↑ 3 METHODS AND MATERIALS Figure 1 outlines the system’s systematic process. This study builds upon the architecture of the previous study conducted by [2], with the addition of a key component: capturing and saving the coordi- nates of the bounding boxes annotated by the radiologist for record keeping purposes. Figure 1: Conceptual Framework The system workflow begins with the radiologist logging in and uploading a chest X-ray (CXR) image. They manually annotate the image and click the “Save Annotations” button after each an- notation, saving the bounding box coordinates in a CSV file for record-keeping. Simultaneously, the AI model generates automated annotations on the uploaded image. Once the annotations are com- plete, the radiologist proceeds to the next page to generate the radiographic report. Finally, they click the "Download & Print" button to generate and download the report. 3.1 Annotation Tool and Classification Model The annotation model, SSD300_VGG16, processes chest X-ray im- ages by localizing and classifying abnormalities. Integrated into the annotation platform, it accelerates the process by providing initial automated annotations for radiologists to review and refine. The system supports radiologists by allowing image uploads and two an- notation methods: automated and manual. Automated annotation, powered by SSD300_VGG16, detects and categorizes abnormalities with bounding boxes, while manual annotation enables radiolo- gists to add bounding boxes, labels, and zoom for detailed analysis. The tool consolidates findings into reports for record-keeping and review, combining AI efficiency with expert precision to enhance workflow. 3.2 Data Collection and Preprocessing We utilized chest X-ray datasets from public sources, namely the NIH dataset and the VinBig dataset, which were combined after meticulous preprocessing to ensure the quality and relevance of the data. From the NIH dataset, an initial pool of 112,120 images was selected, with only 985 images retained due to their accom- panying bounding box annotations and classifications in a CSV format. For the VinBig dataset, which includes 15,000 training im- ages annotated by multiple radiologists, Weighted Box Fusion was applied to consolidate annotations. After excluding images without abnormalities and those irrelevant to the nine selected categories,

PCSC2025, May 2025, Baguio, Philippines approximately 4,000 images were incorporated. Following further filtering, the combined dataset was reduced to 4,718 images. To address data imbalance among classes, optimized sampling methods were employed. For example, anomalies with fewer in- stances, such as Pneumothorax (171 samples) and Atelectasis (217 samples), were given priority during data preparation. After balanc- ing the dataset, 3,500 images were designated for training, divided into 80% training and 20% validation sets using stratified sampling. In addition, a structured preprocessing pipeline was implemented for compatibility with the SSD300_VGG16 model. Images were pre- processed to fit the Pascal VOC format, ensuring they met the input size requirements (300×300 pixels). They were then organized into a VOCdevkit directory structure, with subfolders for annotations, image sets, and PNG images. Bounding box annotations were con- verted into XML format, adhering to the VOC standard to ensure compatibility with the SSD300_VGG16 pipeline. Finally, training and validation images were indexed into text files to streamline the model’s data loading process. 3.3 Model Preparation and Integration 3.3.1 Data Preparation. We started with 880 NIH chest X-ray images containing bound- ing box coordinates and categorized them into nine abnormalities: pulmonary fibrosis, pleural thickening, infiltration, cardiomegaly, pleural effusion, nodule/mass, consolidation, atelectasis, and pneu- mothorax. To ensure consistency, we selected only images with a posterior-to-anterior (PA) view, leaving us with 692 images. In the VinBig dataset, since all are PA view, we immediately sorted the X-ray images into the same nine abnormality categories and removed those without abnormalities. Because the VinBig dataset includes annotations from multiple annotators, we applied Weighted Box Fusion—a preprocessing technique that combines predictions from different radiologists—which resulted in 4,158 images. In total, 4,850 images utilized both from NIH and VinBig datasets. We resized the original images from 1024×1024 pixels to 300×300 pixels and scaled the bounding boxes accordingly to meet our model’s specifications. Since an X-ray image can have multiple abnormalities, there are 9,613 samples in total, distributed as follows: Cardiomegaly: 2,405; Pleural thickening: 1,981; Pulmonary Fibrosis: 1,617; Pleural effusion: 1,173; Nodule/Mass: 960; Infiltration: 613; Consolidation: 353; Atelectasis: 340; and Pneumothorax: 171. 3.3.2 Data Distribution. Iteration 1 and Iteration 2: From the 9,613 samples, optimized balanced sampling was applied to address class imbalance, resulting in 3,518 samples. The dataset was then partitioned into training (80%) and validation (20%) sets using stratified sampling to ensure an even distribution of abnormalities across both sets. The final distribution of samples is shown in Table 1 below: Table 2: Data Distribution for Iteration 1 and Iteration 2 Abnormalities No. of Samples Pleural thickening 671 Pulmonary fibrosis 591 Pleural effusion 487 Cardiomegaly 469 Nodule/Mass 363 Infiltration 306 Consolidation 243 Atelectasis 217 Pneumothorax 171 The dataset was partitioned into training and validation sets, with 80% designated for training and 20% for validation. Stratified sampling was used to ensure that abnormalities were evenly dis- tributed across both sets. The data distribution remained consistent across both iterations, with stratified sampling ensuring a balanced representation in the training and validation sets. Iteration 3: The distribution of data within the training set was changed to six classes and applied optimized balance sampling re- sulting in 6,082 samples. The distribution is detailed in Table 2: Table 3: Data Distribution for Iteration 3 Abnormalities No. of Samples Pleural thickening 1408 Pulmonary fibrosis 1235 Cardiomegaly 1176 Pleural effusion 901 Nodule/Mass 749 Infiltration 613 Similar to the previous iterations, stratified sampling was used to partition the samples into an 80% training set and a 20% validation set. 3.4 Model Architecture Figure 2: Visualization of Model using visualkeras

Automatic Chest X-Ray Pattern Annotation and Classification PCSC2025, May 2025, Baguio, Philippines The SSD300_VGG16 model was employed for chest X-ray abnor- mality detection, offering a balance between accuracy and real-time performance, making it well-suited for medical imaging tasks. The architecture utilizes a VGG16 backbone for hierarchical feature ex- traction and incorporates components for class prediction and box localization, both essential for detecting and localizing abnormali- ties in chest X-rays. The class predictor identifies abnormalities in detected regions, with a design that integrates VGG16 for feature ex- traction, followed by a classification head that outputs probabilities across nine abnormality classes. Softmax is used as the activation function to generate a probability distribution over these classes, while focal loss is applied as the training objective to handle class imbalance and focus on hard-to-classify examples. Meanwhile, the box predictor localizes abnormalities by predicting bounding boxes, using a regression head that estimates bounding box coordinates (x_center, y_center, width, height), scaled to the image dimensions. Instead of using corner-based representation, bounding boxes are predicted as center, width, and height, with Smooth L1 loss serving as the training objective to optimize bounding box predictions. 3.5 Capturing and Saving the Coordinates of the Bounding Boxes To capture and save the coordinates of bounding boxes, the re- searchers integrated the annotation tool used by [2], which allows radiologists to draw bounding boxes around abnormalities in chest X-ray images. This tool was enhanced to extract these coordinates and store them in a CSV file. The CSV file includes columns for the image name, classification of the abnormality, and bounding box coordinates of the annotations. 3.6 Automatic Interpretation The researchers used a template-based approach to generate de- tailed, sentence-form descriptions of radiographic findings. A pre- designed template was created to ensure that the reports are clear, standardized, and coherent. These template included predefined phrases and structures tailored to describe common abnormalities and their locations, allowing for efficient and consistent report generation. This approach simplifies the process by providing a structured framework for creating radiographic reports without requiring complex algorithms. 4 RESULTS AND DISCUSSIONS 4.1 Model Performance The object detection model was evaluated using Average Precision (AP) and Average Recall (AR) across different IoU thresholds and image area sizes. Higher AP and AR values indicate better perfor- mance. IoU measures the overlap between predicted and actual bounding boxes, while maxDets defines the maximum number of detections per image. Table 3 presents the AP and AR values for each iteration. Table 4: Performance Metrics Table Iteration 1 Iteration 2 Iteration 3 Overall Precision AP @ [IoU=0.50:0.95] 0.0092 0.0079 0.3102 Precision at Specific IoU Thresholds AP @ [IoU=0.50] 0.0217 0.0185 0.3456 AP @ [IoU=0.75] 0.0051 0.0053 0.3309 Precision by Area Size Small 0.0011 0.0013 0.3791 Medium 0.0102 0.0105 0.3251 Large 0.0110 0.0091 0.3477 Recall by Area Size Small 0.1043 0.0854 0.8358 Medium 0.1599 0.1179 0.9031 Large 0.1582 0.1175 0.9141 Recall Analysis AR@1 0.1138 0.1021 0.8855 AR@10 0.1138 0.1274 0.8866 AR@100 0.1656 0.1274 0.8866 4.2 Iteration 1 The model was initialized with ImageNet-pretrained weights, using a batch size of 16 for training and 1 for validation. The learning rate was set to 0.0001, with a momentum of 0.9 and a weight decay of 0.0005. At the 150th epoch, the model’s loss metrics were as follows: a classification loss of 2.0204, a localization loss of 0.0781, and a total loss of 2.0985. While the classification loss indicated the need for further optimization, the low localization loss demonstrated effec- tive abnormality detection. These early results suggest that further refinements could enhance the model’s diagnostic capabilities. 4.2.1 AUC Scores. The Area Under the Curve (AUC) scores for each class or abnor- mality are as follows: Table 5: AUC Scores for Iteration 1 Abnormalities AUC Scores Cardiomegaly 0.62 Pleural Thickening 0.32 Pulmonary fibrosis 0.40 Pleural effusion 0.44 Nodule/Mass 0.53 Infiltration 0.48 Consolidation 0.56 Atelectasis 0.58 Pneumothorax 0.61 The AUC metric evaluates binary classifiers by summarizing performance across all thresholds via the ROC curve, where 1 is

PCSC2025, May 2025, Baguio, Philippines perfect and 0.5 is random. In our detection task, AUC scores varied by abnormality: Consolidation (0.63) and Cardiomegaly (0.62) per- formed best, followed by Pneumothorax (0.59); Atelectasis (0.53) and Nodule/Mass (0.52) were moderate; Infiltration (0.49) was bor- derline; and Pulmonary Fibrosis (0.40), Pleural Effusion (0.42), and Pleural Thickening (0.31) showed limited detection, indicating a need for further improvement. Figure 3: Model Output Comparison for Iteration 1 4.2.2 Output Comparison. After applying non-maximum sup- pression, it is evident that the model has localization issues. The model identified Nodule/Mass, Cardiomegaly, and Pleural Thicken- ing, whereas the correct abnormalities are Infiltration, Cardiomegaly, and Pleural Thickening. Although it correctly detected two abnor- malities, it struggled with accurate localization. 4.2.3 Annotation Tool. A "Save Annotations" button was added to the annotation tool, allowing radiologists to store bounding box data in CSV format. This ensures annotations are saved only when finalized, preventing data loss and improving workflow. A confirmation notification verifies successful saving. 4.2.4 Radiographic Report. A dynamic input form was added to the radiographic report, allow- ing users to enter patient details (name, ID, gender, and age), which automatically updates in the report. Additionally, a date and time feature and a Recommendations section were introduced to sug- gest follow-up actions like CT scans, MRIs, or clinical evaluations. However, the Recommendations section currently lacks predefined phrases and data input functionality. 4.3 Iteration 2 4.3.1 Implementation. The model was initialized with ImageNet-pretrained weights, a batch size of 32 for training and 1 for validation, and was optimized using Stochastic Gradient Descent (SGD) with a learning rate of 0.001, momentum of 0.9, and a weight decay of 0.0005. At the 300th epoch, the model’s loss metrics were as follows: the classification loss was 1.88699297, the localization loss was 0.06171739, and the total loss was 1.94871035. In the second iteration, the classification loss dropped from 2.02 to 1.89 and the localization loss from 0.078 to 0.062, reducing the total loss from 2.10 to 1.95. These improvements indicate better classification and localization, demonstrating the positive impact of the adjustments and the potential for further refinement. 4.3.2 AUC Scores. The Area Under the Curve (AUC) scores for each class or abnor- mality are as follows: Table 6: AUC Scores for Iteration 1 Abnormalities AUC Scores Cardiomegaly 0.62 Pleural Thickening 0.31 Pulmonary fibrosis 0.40 Pleural effusion 0.42 Nodule/Mass 0.52 Infiltration 0.49 Consolidation 0.53 Atelectasis 0.63 Pneumothorax 0.59 The AUC scores vary by abnormality. Consolidation (0.63), Car- diomegaly (0.62), and Pneumothorax (0.59) show moderate relia- bility, while Atelectasis (0.53), Nodule/Mass (0.52), and Infiltration (0.49) are moderate. The model struggles with Pulmonary Fibrosis (0.40) and Pleural Effusion (0.42), and Pleural Thickening (0.31) is barely above random chance. Overall, significant improvements are needed for abnormalities with AUCs below 0.5. Figure 4: Model Output Comparison for Iteration 2 4.3.3 Output Comparison. After applying non-maximum sup- pression, it is evident that the model has a problem with localiza- tion. The model identified Nodule/Mass, Cardiomegaly, and Pleural Thickening. However, the correct abnormalities are Infiltration, Cardiomegaly, and Pleural Thickening. It might have identified two abnormalities, but it has a hard time localizing them.

Automatic Chest X-Ray Pattern Annotation and Classification PCSC2025, May 2025, Baguio, Philippines 4.3.4 Annotation Tool. In Iteration 2, the Annotation Tool’s user interface was simplified by removing the sub-abnormalities dropdown and limiting the ab- normalities list to nine options, improving processing time but reducing flexibility. A "No Abnormality" option was added, though it may cause confusion for some users. Additionally, a "Save AI Im- age" button was introduced to allow manual saving, addressing the system’s current limitation in dynamically displaying AI-generated images on the next page. 4.3.5 Radiographic Report. The radiographic report was expanded to include Clinical Indication, Findings, and Impression sections, providing context for imaging exams, detailing anatomical observations, and summarizing clinical significance. However, these sections lack predefined phrases and input functionality, serving as a foundation for future development. 4.4 Iteration 3 4.4.1 Implementation. The model was initialized with ImageNet-pretrained weights, a batch size of 64 for training 1 for validation, and Stochastic Gradient Descent (SGD) with learning rate of 0.001, momentum of 0.9, and weight decay 0.0005. Figure 5: Total Loss Over Epochs for Iteration 3 At the 260th epoch, the model’s loss metrics were as follows: classification loss of 1.96, localization loss of 0.0449, and a total loss of 2.01. While the classification loss indicates a need for further optimization, the low localization loss suggests effective abnormal- ity detection. Additional refinements could further improve the model’s diagnostic performance. 4.4.2 AUC Scores. The Area Under the Curve (AUC) scores for each class or abnor- mality are as follows: Figure 6: AUC Scores for Iteration 3 For Iteration 3, the model demonstrated strong performance in detecting Cardiomegaly (0.86), Pleural effusion (0.83), and Infiltra- tion (0.90), with AUC scores above 0.80. Moderate effectiveness was observed for Pleural thickening (0.71) and Pulmonary fibrosis (0.78), indicating the need for targeted improvements in classifying these abnormalities. 4.4.3 Output Comparison. Figure 7: Model Output Comparison for Iteration 3 After applying non-maximum suppression, instead of correctly identifying Infiltration, the model detects Nodule/Mass, even though it accurately recognizes Cardiomegaly and Pleural Thickening. This suggests that while the model can detect multiple abnormalities, it struggles with precise localization. 4.4.4 Annotation Tool. The abnormalities list was expanded, the "No Abnormality" op- tion was removed, and the sub-abnormality dropdown was rein- stated based on feedback from the research adviser and radiologist.

PCSC2025, May 2025, Baguio, Philippines The system now automatically saves and displays AI-generated images, eliminating the manual save feature, though users must refresh the page to view newly uploaded expert-annotated images in reports. Additionally, the Iteration 2 model was integrated to detect atelectasis, pneumothorax, and consolidation, as these ab- normalities were excluded from training in Iteration 3, ensuring the annotation tool covers all nine abnormalities in the study. 4.4.5 Radiographic Report. The radiographic report was improved by moving patient de- tails input to the report generation page and adding a modal form with predefined dropdown phrases for key sections, enhancing efficiency and consistency. The expert report includes an editable modal feature, while the AI report automatically incorporates de- tected abnormalities. To ensure standardization and accuracy, the system integrates RSNA RadReport templates, which provide pre- defined phrases for key report sections. These predefined phrases help clearly describe abnormality locations, ensuring consistent and accurate documentation. In addition to RSNA RadReport templates, researchers manually refined these phrases to improve precision. These enhancements collectively streamline reporting, enhance efficiency, and optimize the annotation tool’s functionality. Fig- ures 9 and 10 illustrate the radiographic report template for both expert-annotated and AI-annotated cases. Figure 8: Expert-Annotated Radiographic Report Figure 9: AI-Annotated Radiographic Report 5 CONCLUSION This study achieved significant advancements in chest X-ray ab- normality detection through an iterative approach, refining both the object detection model and annotation tool across three phases. These improvements enhanced performance and user experience in medical image analysis. (1) Annotation Tool Enhancement: The system now allows radiologists to save annotations, preserving expert-annotated images and bounding box coordinates in a CSV file. This feature is fully operational, improving data integrity and usability. (2) Abnormality Classification: CXRaide 2.0 demonstrated notable improvements over CXRaide 1.0, reducing misclas- sification and improving focus on relevant areas. However, detection errors persist, requiring further testing, additional training, and expanded datasets to enhance reliability. (3) Automated Findings Interpretation: Generating accurate sentence-based descriptions remains challenging due to the complexity of defining clear, standardized phrases. While predefined phrases were integrated to generate templated sentences, further refinement is needed for a more compre- hensive radiology reporting system. REFERENCES [1] Y. Akhter, R. Singh, and M. Vatsa. 2023. AI-Based Radiodiagnosis Using Chest X-Rays: A Review. Frontiers in Big Data 6 (2023), 1120989. https://doi.org/10. 3389/fdata.2023.1120989 [2] K. V. Bersamin, K. J. Shitan, J. J. B. Raper, and K. M. Adlaon. 2024. Chest X-Ray Annotation Optimisation Model Using Weighted Boxes Fusion. In Proceedings of the 2023 6th International Conference on Digital Medicine and Image Processing (Kyoto, Japan) (DMIP ’23). Association for Computing Machinery, New York, NY, USA, 88–93. https://doi.org/10.1145/3637684.3637711 [3] M. Danu, G. Marica, S. Karn, B. Georgescu, F. Ghesu, L. M. Itu, C. Suciu, S. Grbic, D. Farri, and D. Comaniciu. 2023. Generation of Radiology Findings in Chest X-Ray by Leveraging Collaborative Knowledge. (06 2023). https: //doi.org/10.48550/arXiv.2306.10448 [4] S. Ganesan and T. S. Subashini. 2014. Classification of medical x-ray images for automated annotation. Journal of Theoretical and Applied Information Technology 63, 3 (2014). [5] Y. Han, C. Chen, A. Tewfik, B. Glicksberg, Y. Ding, Y. Peng, and Z. Wang. 2022. Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-Rays with Radiomics Using a Feedback Loop. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2465–2474. https://openaccess.thecvf.com/content/WACV2022/ papers/Han_Knowledge-Augmented_Contrastive_Learning_for_Abnormality_ Classification_and_Localization_in_Chest_WACV_2022_paper.pdf [6] J. N. Itri, R. R. Tappouni, R. O. McEachern, A. J. Pesch, and S. H. Patel. 2018. Fundamentals of Diagnostic Error in Imaging. Radiographics 38, 6 (2018), 1845– 1865. https://doi.org/10.1148/rg.2018180033 [7] Z. P. Jiang, Y. Y. Liu, Z. E. Shao, and K. W. Huang. 2021. An Improved VGG16 Model for Pneumonia Image Classification. Applied Sciences 11, 23 (2021), 11185. https://doi.org/10.3390/app112311185 [8] Navdeep Kaur and Ajay Mittal. 2022. RadioBERT: A deep learning-based sys- tem for medical report generation from chest X-ray images using contextual embeddings. Journal of Biomedical Informatics 135 (2022), 104220. https: //doi.org/10.1016/j.jbi.2022.104220 [9] P Mehrotra, V Bosemani, and J Cox. 2009. Do radiologists still need to report chest x rays? Postgraduate medical journal 85, 1005 (2009), 339–341. https://www.researchgate.net/profile/Julie_Cox3/publication/ 26651061_Do_radiologists_still_need_to_report_chest_x_rays/links/ 542ff8d80cf27e39fa99ce62/Do-radiologists-still-need-to-report-chest-x-rays. pdf [10] T. Pang, P. Li, and L. Zhao. 2023. A survey on automatic generation of medical imaging reports based on deep learning. BioMedical Engineering OnLine 22, 1 (2023), 48. https://doi.org/10.1186/s12938-023-01113-y

Automatic Chest X-Ray Pattern Annotation and Classification PCSC2025, May 2025, Baguio, Philippines [11] K. J. P. Shitan, J. J. B. A. Raper, K. V. F. Bersamin, and K. M. M. Adlaon. 2024. AI-Assisted Chest X-ray Annotation Tool for Abnormality Classifica- tion and Localization. In Proceedings of the Philippine Computing Science Con- gress (PCSC) Student Research Workshop. https://pcsc.dlsu.edu.ph/proceedings/ student-research-workshop/51.pdf [12] A. Smit, S. Jain, P. Rajpurkar, A. Pareek, A. Y. Ng, and M. P. Lungren. 2020. CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT. arXiv preprint arXiv:2004.09167 (2020). [13] Y. Vasilev, A. Vladzymyrskyy, O. Omelyanskaya, I. Blokhin, Y. Kirpichev, and K. Arzamasov. 2023. AI-Based CXR First Reading: Current Limitations to En- sure Practical Value. Diagnostics 13, 8 (2023), 1430. https://doi.org/10.3390/ diagnostics13081430 [14] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M. Summers. 2018. TieNet: Text-Image Embedding Network for Common Thorax Disease Classifi- cation and Reporting in Chest X-Rays. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . 9049–9058. [15] M. J. Willemink, W. A. Koszek, C. Hardell, J. Wu, D. Fleischmann, H. Harvey, and M. P. Lungren. 2020. Preparing Medical Imaging Data for Machine Learning. Radiology 295, 1 (2020), 4–15. https://doi.org/10.1148/radiol.2020192224 [16] Junde Wu, Yuval Gur, Alexandros Karargyris, A. Bilal Syed, Oleg Boyko, Mehran Moradi, and Tanveer Syeda-Mahmood. 2020. Automatic bounding box annotation of chest x-ray data for localization of abnormalities. In2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI) . IEEE, 799–803. [17] F. Yan, X. Huang, Y. Yao, M. Lu, and M. Li. 2019. Combining LSTM and DenseNet for Automatic Annotation and Classification of Chest X-ray Images. IEEE Access 7 (2019), 74181–74189. [18] M. R. Zare, A. Mueen, and W. C. Seng. 2014. Automatic Medical X-ray Image Classification Using Annotation. Journal of Digital Imaging 27 (2014), 77–89.